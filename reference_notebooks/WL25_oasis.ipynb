{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3f0f6c8-0780-44d8-8b60-67c3f4326b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import lstsq\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import path\n",
    "path.append('..')\n",
    "from oasis.functions import gen_data, gen_sinusoidal_data, deconvolve, estimate_parameters\n",
    "from oasis.plotting import simpleaxis\n",
    "from oasis.oasis_methods import oasisAR1, oasisAR2\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "#%matplotlib inline\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005202bc-15ca-4dd5-9d23-2ceda574edd5",
   "metadata": {},
   "source": [
    "## Load data low-pass filtered Minian raw output (no temporal deconvolution update), zarr format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6c49d7b-9d70-48d1-a99f-c8ff450c1893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before curation: Frozen({'unit_id': 114, 'frame': 6000})\n",
      "Keeping 3 curated units out of 3\n",
      "Curated unit IDs: [9, 10, 11]\n",
      "After curation: Frozen({'unit_id': 3, 'frame': 6000})\n",
      "Vars: ['C_lp']\n",
      "Shape: (3, 6000) | Dims: ('unit_id', 'frame')\n",
      "Units: 3 Frames: 6000\n"
     ]
    }
   ],
   "source": [
    "# --- load filtered traces (all units) ---\n",
    "dpath = \"./minian_intermediate\"\n",
    "\n",
    "C_ds = xr.open_zarr(f\"{dpath}/C_lowpass.zarr\", consolidated=False)\n",
    "C_all = C_ds[\"C_lp\"].astype(np.float32)\n",
    "\n",
    "print(\"Before curation:\", C_all.sizes)\n",
    "\n",
    "# --- load your curation CSV ---\n",
    "curation_path = \"./curation_results.csv\"\n",
    "\n",
    "curation_df = pd.read_csv(curation_path)\n",
    "keep_ids = curation_df.loc[curation_df[\"keep\"] == 1, \"unit_id\"].values\n",
    "\n",
    "print(f\"Keeping {len(keep_ids)} curated units out of {len(curation_df)}\")\n",
    "print(\"Curated unit IDs:\", keep_ids.tolist())\n",
    "\n",
    "# --- filter down to curated units ---\n",
    "C = C_all.sel(unit_id=keep_ids)\n",
    "\n",
    "print(\"After curation:\", C.sizes)\n",
    "\n",
    "\n",
    "print(\"Vars:\", list(C_ds.data_vars))\n",
    "print(\"Shape:\", C.shape, \"| Dims:\", C.dims)\n",
    "print(\"Units:\", C.sizes[\"unit_id\"], \"Frames:\", C.sizes[\"frame\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81903896-750f-472e-b7ba-f8823f476ada",
   "metadata": {},
   "source": [
    "## Implementing functions to measure rise and fall time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82230ff8-b0ae-4518-87fe-88c67bbe2920",
   "metadata": {},
   "source": [
    "estimates rise times and decay time constants (τ) from calcium imaging data (e.g., GCaMP6f). The goal is to extract biophysically meaningful kinetics while staying robust to noise and outliers\n",
    "\n",
    "1. Event Detection\n",
    "dettrend trace against baseline\n",
    "\n",
    "- estimate noise (MAD)\n",
    "- peaks are called using `scipy.find_peaks`\n",
    "- which requires **Promience** and **Seperation**\n",
    "- extract a small window around the event (pre + post)\n",
    "\n",
    "2. Rise Time (10%-90%)\n",
    "- fluorescence trace is normalized to peak amplitude\n",
    "- find the time when the trace crosses **10%** and **90%** of the peak.\n",
    "- Checks: requires both crossings to exist and be in the correct order\n",
    "   \n",
    "3. Decay Constant\n",
    "- fit the falling phase with a log-linear exponential mode\n",
    "- Two options for when to stop fitting:\n",
    "  \n",
    "      tail_to=\"baseline\" → fit until trace returns near baseline\n",
    "      tail_to=\"frac\" → fit until a fixed fraction of peak (e.g., 30%)\n",
    "  \n",
    "  \n",
    "- avoid over-fitting noise:\n",
    "  \n",
    "       Stop fitting once signal < noise_k × MAD.\n",
    "\t   Require at least min_tail_pts samples.\n",
    "\t   Only accept fits with R² ≥ r2_min on the log-scale.\n",
    "- apply tau_cap to drop or clip unrealistically long decays\n",
    "  \n",
    "4. Outlier & Noise Handling\n",
    "- MAD multiplied with noise_k. Prevents fitting tails eblow detetcable level\n",
    "- Goddness of fit R², rject fits that dont look exponetial\n",
    "- tau_cap avoids rare run-away fits biasing the median\n",
    "- max_events_per_unit keeps plots readable and stats balanced\n",
    "\n",
    "   \n",
    "5. Group summary\n",
    "- Reported statistics\n",
    "- Median ± IQR (25–75%) for rise and decay.\n",
    "- Count of valid events contributing to each.\n",
    "- These can then be converted into AR(2) parameters (g) for deconvolution (OASIS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b66f6cc4-1bad-4abc-9f97-89df9d27d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_event_kinetics(\n",
    "    t, y, peak_idx, base_val,\n",
    "    rise_lo=0.10, rise_hi=0.90,\n",
    "    tail_to=\"baseline\",          # \"baseline\" (preferred) or \"frac\"\n",
    "    tail_frac=0.10,              # only used if tail_to == \"frac\"\n",
    "    min_tail_pts=8,              # need enough points to fit a line\n",
    "    r2_min=0.85,                 # require decent log-linear fit\n",
    "    noise_k=2.0,                 # exclude tail once below noise floor = k * MAD\n",
    "    max_tau_s=None               # e.g., 3.0 to drop >3s, or None to keep all\n",
    "):\n",
    "    \"\"\"\n",
    "    Measure rise time (10–90%) and decay time constant τ for one event window.\n",
    "\n",
    "    t, y       : 1D arrays for the event window (seconds and fluorescence)\n",
    "    peak_idx   : index (in this window) of the event peak\n",
    "    base_val   : baseline value for this unit/segment (same units as y)\n",
    "\n",
    "    Returns:\n",
    "      dict(rise_t10_90_s, tau_decay_s, A_peak, ok_rise, ok_tau)\n",
    "    \"\"\"\n",
    "    # amplitude at peak\n",
    "    A = float(y[peak_idx] - base_val)\n",
    "    if not np.isfinite(A) or A <= 1e-9:\n",
    "        return dict(rise_t10_90_s=np.nan, tau_decay_s=np.nan, A_peak=0.0,\n",
    "                    ok_rise=False, ok_tau=False)\n",
    "\n",
    "    # ---------- Rise (10→90%) ----------\n",
    "    yn = (y - base_val) / A\n",
    "    tn = t\n",
    "\n",
    "    def _t_at_level(tseg, yseg, level):\n",
    "        above = yseg >= level\n",
    "        if not np.any(above):\n",
    "            return np.nan\n",
    "        i = np.argmax(above)\n",
    "        if i == 0:\n",
    "            return tseg[0]\n",
    "        # linear interpolation between i-1 and i\n",
    "        t0, t1 = tseg[i-1], tseg[i]\n",
    "        y0, y1 = yseg[i-1], yseg[i]\n",
    "        if y1 == y0:\n",
    "            return t1\n",
    "        return t0 + (level - y0) * (t1 - t0) / (y1 - y0)\n",
    "\n",
    "    t10 = _t_at_level(tn[:peak_idx+1], yn[:peak_idx+1], rise_lo)\n",
    "    t90 = _t_at_level(tn[:peak_idx+1], yn[:peak_idx+1], rise_hi)\n",
    "    ok_rise = np.isfinite(t10) and np.isfinite(t90) and (t90 > t10)\n",
    "    rise_t = (t90 - t10) if ok_rise else np.nan\n",
    "\n",
    "    # ---------- Decay τ (baseline-to-peak tail) ----------\n",
    "    # Use the original (not normalized) trace for stability.\n",
    "    z = np.maximum(y - base_val, 0.0)\n",
    "\n",
    "    # noise floor from pre-peak tail: MAD × 1.4826\n",
    "    pre = z[max(0, peak_idx-20):peak_idx]\n",
    "    mad = np.median(np.abs(pre - np.median(pre))) * 1.4826 if pre.size else 0.0\n",
    "    noise_floor = noise_k * mad\n",
    "\n",
    "    # decide tail stop value\n",
    "    if tail_to == \"baseline\":\n",
    "        stop_level = max(noise_floor, tail_frac * A)  # don't go below noise\n",
    "    else:\n",
    "        stop_level = max(tail_frac * A, noise_floor)\n",
    "\n",
    "    # tail region: from peak_idx forward until z <= stop_level\n",
    "    end = peak_idx + 1\n",
    "    while end < len(z) and z[end] > stop_level:\n",
    "        end += 1\n",
    "\n",
    "    # need enough samples\n",
    "    if (end - peak_idx) < min_tail_pts:\n",
    "        return dict(rise_t10_90_s=rise_t, tau_decay_s=np.nan, A_peak=A,\n",
    "                    ok_rise=ok_rise, ok_tau=False)\n",
    "\n",
    "    tt = t[peak_idx:end]\n",
    "    zz = z[peak_idx:end]\n",
    "\n",
    "    # ensure strictly positive for log\n",
    "    mask = zz > max(noise_floor, 1e-9)\n",
    "    if np.count_nonzero(mask) < min_tail_pts:\n",
    "        return dict(rise_t10_90_s=rise_t, tau_decay_s=np.nan, A_peak=A,\n",
    "                    ok_rise=ok_rise, ok_tau=False)\n",
    "\n",
    "    tt = tt[mask]\n",
    "    zz = zz[mask]\n",
    "\n",
    "    # log-linear fit: log(zz) = c - (1/τ) * t\n",
    "    X = np.vstack([np.ones_like(tt), -tt]).T\n",
    "    ylog = np.log(zz)\n",
    "    # least squares\n",
    "    beta, *_ = lstsq(X, ylog, rcond=None)   # [c, 1/τ]\n",
    "    inv_tau = beta[1]\n",
    "    if inv_tau <= 0 or not np.isfinite(inv_tau):\n",
    "        return dict(rise_t10_90_s=rise_t, tau_decay_s=np.nan, A_peak=A,\n",
    "                    ok_rise=ok_rise, ok_tau=False)\n",
    "    tau = 1.0 / inv_tau\n",
    "\n",
    "    # R^2 on log scale to verify exponentiality\n",
    "    yhat = X @ beta\n",
    "    ss_res = np.sum((ylog - yhat)**2)\n",
    "    ss_tot = np.sum((ylog - np.mean(ylog))**2) + 1e-12\n",
    "    r2 = 1.0 - ss_res/ss_tot\n",
    "\n",
    "    ok_tau = np.isfinite(tau) and (tau > 0) and (r2 >= r2_min)\n",
    "    if not ok_tau:\n",
    "        tau = np.nan\n",
    "\n",
    "    # optional cap/drop for outliers\n",
    "    if ok_tau and (max_tau_s is not None) and (tau > max_tau_s):\n",
    "        # Drop (return NaN) rather than clamp to avoid biasing medians upward\n",
    "        tau = np.nan\n",
    "        ok_tau = False\n",
    "\n",
    "    return dict(rise_t10_90_s=rise_t, tau_decay_s=tau, A_peak=A,\n",
    "                ok_rise=ok_rise, ok_tau=ok_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4c0cf5f-a914-45a7-847b-6a17d813b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- helpers ----------\n",
    "def robust_baseline(y, q=10):\n",
    "    return np.percentile(y, q)\n",
    "\n",
    "def simple_lowpass(y, wlen_sec=0.3, fps=20.0):\n",
    "    w = max(3, int(round(wlen_sec * fps)) | 1)   # odd window\n",
    "    k = np.ones(w, dtype=float) / w\n",
    "    return np.convolve(y, k, mode=\"same\")\n",
    "\n",
    "def detect_isolated_events(y, fps, min_prom_sigma=3.5, min_separation_s=1.2,\n",
    "                           pre_s=0.5, post_s=2.0, smooth_sec=None):\n",
    "    \"\"\"\n",
    "    Returns (events, baseline), where events is a list of (start, peak, end) indices.\n",
    "    min_prom_sigma is relative to a robust noise estimate (MAD).\n",
    "    \"\"\"\n",
    "    if smooth_sec is not None:\n",
    "        y_use = simple_lowpass(y, wlen_sec=smooth_sec, fps=fps)\n",
    "    else:\n",
    "        y_use = y\n",
    "\n",
    "    base = robust_baseline(y_use, q=10)\n",
    "    z = y_use - base\n",
    "\n",
    "    # robust noise estimate\n",
    "    mad = np.median(np.abs(z - np.median(z))) + 1e-12\n",
    "    sig = 1.4826 * mad\n",
    "    prom = max(1e-6, float(min_prom_sigma) * sig)\n",
    "\n",
    "    distance = int(round(min_separation_s * fps))\n",
    "    peaks, props = find_peaks(z, prominence=prom, distance=distance)\n",
    "\n",
    "    events = []\n",
    "    pre = int(round(pre_s * fps))\n",
    "    post = int(round(post_s * fps))\n",
    "    n = len(y)\n",
    "    for p in peaks:\n",
    "        s = max(0, p - pre)\n",
    "        e = min(n, p + post)\n",
    "        if e - s >= 5:\n",
    "            events.append((s, p, e))\n",
    "    return events, base\n",
    "\n",
    "def frac_time_to_level(t, y, level):\n",
    "    above = y >= level\n",
    "    if not np.any(above):\n",
    "        return np.nan\n",
    "    idx = np.argmax(above)\n",
    "    if idx == 0:\n",
    "        return t[0]\n",
    "    t0, t1 = t[idx-1], t[idx]\n",
    "    y0, y1 = y[idx-1], y[idx]\n",
    "    if y1 == y0:\n",
    "        return t1\n",
    "    return t0 + (level - y0) * (t1 - t0) / (y1 - y0)\n",
    "\n",
    "def _exp_decay(t, A, tau, B):\n",
    "    return A * np.exp(-(t - t[0]) / max(1e-9, tau)) + B\n",
    "\n",
    "def measure_event_kinetics(t, y, peak_idx, base_val,\n",
    "                           rise_lo=0.1, rise_hi=0.9,\n",
    "                           tail_to=\"baseline\",    # \"baseline\" or \"frac\"\n",
    "                           tail_frac=0.50,        # only used if tail_to==\"frac\"\n",
    "                           min_tail_pts=8, r2_min=0.85,\n",
    "                           noise_k=2.0, max_tau_s=None):\n",
    "    \"\"\"\n",
    "    Compute rise t10→90 and exponential tau on the decay.\n",
    "    Returns dict with keys: rise_t10_90_s, tau_decay_s, ok_rise, ok_tau.\n",
    "    \"\"\"\n",
    "    A = y[peak_idx] - base_val\n",
    "    if A <= 1e-12:\n",
    "        return dict(rise_t10_90_s=np.nan, tau_decay_s=np.nan,\n",
    "                    ok_rise=False, ok_tau=False)\n",
    "\n",
    "    yn = (y - base_val) / A\n",
    "    tn = t\n",
    "\n",
    "    # Rise\n",
    "    t10 = frac_time_to_level(tn[:peak_idx+1], yn[:peak_idx+1], rise_lo)\n",
    "    t90 = frac_time_to_level(tn[:peak_idx+1], yn[:peak_idx+1], rise_hi)\n",
    "    ok_rise = np.isfinite(t10) and np.isfinite(t90) and (t90 > t10)\n",
    "    rise_t = (t90 - t10) if ok_rise else np.nan\n",
    "\n",
    "    # Decide where to stop decay fit\n",
    "    if tail_to == \"baseline\":\n",
    "        # go until the trace returns to near-baseline (use small band around 0)\n",
    "        end_idx = peak_idx + 1\n",
    "        while end_idx < len(yn) and yn[end_idx] > 0.02:\n",
    "            end_idx += 1\n",
    "    else:  # \"frac\"\n",
    "        end_idx = peak_idx + 1\n",
    "        while end_idx < len(yn) and yn[end_idx] > float(tail_frac):\n",
    "            end_idx += 1\n",
    "\n",
    "    ok_tau = False\n",
    "    tau = np.nan\n",
    "    if end_idx - peak_idx >= max(5, min_tail_pts):\n",
    "        tx = tn[peak_idx:end_idx]\n",
    "        yx = y[peak_idx:end_idx]\n",
    "        try:\n",
    "            p0 = (A, 1.0, base_val)\n",
    "            popt, _ = curve_fit(_exp_decay, tx, yx, p0=p0, maxfev=5000)\n",
    "            tau = float(popt[1])\n",
    "            if max_tau_s is not None and np.isfinite(tau) and tau > max_tau_s:\n",
    "                ok_tau = False\n",
    "            else:\n",
    "                # quick R^2 check\n",
    "                yhat = _exp_decay(tx, *popt)\n",
    "                ss_res = np.sum((yx - yhat)**2)\n",
    "                ss_tot = np.sum((yx - np.mean(yx))**2) + 1e-12\n",
    "                r2 = 1.0 - ss_res/ss_tot\n",
    "                ok_tau = (r2 >= r2_min)\n",
    "        except Exception:\n",
    "            ok_tau = False\n",
    "\n",
    "    return dict(rise_t10_90_s=rise_t, tau_decay_s=tau, ok_rise=ok_rise, ok_tau=ok_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef75be0-e448-47bf-b949-ba9bdbaf8729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transients_with_summary(\n",
    "    C, units, fps, *,\n",
    "    n_examples=10,                 # how many random units to show (total)\n",
    "    window=2.0,                    # seconds after peak to include when aligning\n",
    "    min_prom_sigma=3.5,            # -> detect_isolated_events\n",
    "    min_separation_s=1.2,          # -> detect_isolated_events\n",
    "    smooth_sec=None,               # -> detect_isolated_events (None if C already low-pass)\n",
    "    max_events_per_unit=None,      # cap #events per unit in plots/stats\n",
    "    random_seed=0,                 # for reproducible event subsampling / unit picks\n",
    "\n",
    "    # KINETICS (passed to measure_event_kinetics for every event)\n",
    "    tail_to=\"baseline\",            # \"baseline\" or \"frac\"\n",
    "    tail_frac=0.30,                # used iff tail_to==\"frac\"\n",
    "    min_tail_pts=8,\n",
    "    r2_min=0.85,\n",
    "    noise_k=2.0,\n",
    "    tau_cap=None,                  # e.g. 3.0 (seconds). If None, no cap/drop.\n",
    "    drop_capped=True,              # if True drop tau>cap; else clip to cap\n",
    "\n",
    "    # SUMMARY RETURN\n",
    "    return_stats=False,            # if True, return summary dict(s)\n",
    "    summarize=\"per_group\"          # \"per_group\" or \"all\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot aligned calcium transients + separate rise/decay histograms for random units.\n",
    "    Requires helpers:\n",
    "      - detect_isolated_events(y, fps, ...)\n",
    "      - measure_event_kinetics(t, y, peak_idx, base_val, ...)\n",
    "\n",
    "    Returns:\n",
    "      None, or list of summary dicts if return_stats=True.\n",
    "    \"\"\"\n",
    "    assert summarize in (\"per_group\", \"all\")\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "\n",
    "    # pick units and split into 2 figures for readability\n",
    "    chosen_units = np.random.choice(units, size=n_examples, replace=False)\n",
    "    groups = np.array_split(chosen_units, 2)\n",
    "\n",
    "    summaries = []\n",
    "    all_rise, all_decay = [], []\n",
    "\n",
    "    for fig_idx, group in enumerate(groups, start=1):\n",
    "        # --- three columns per unit: aligned • rise-only • decay-only ---\n",
    "        fig, axes = plt.subplots(len(group), 3, figsize=(15, 2.6*len(group)))\n",
    "        if len(group) == 1:\n",
    "            axes = np.array([axes])  # ensure 2D\n",
    "\n",
    "        # per-figure accumulators (for suptitle + optional per_group stats)\n",
    "        rise_all, decay_all = [], []\n",
    "\n",
    "        for i, uid in enumerate(group):\n",
    "            y = C.sel(unit_id=int(uid)).values.astype(float)\n",
    "            t = np.arange(len(y)) / float(fps)\n",
    "\n",
    "            # detect candidate events (returns list of (start, peak, end) idx)\n",
    "            events, base = detect_isolated_events(\n",
    "                y, fps,\n",
    "                min_prom_sigma=min_prom_sigma,\n",
    "                min_separation_s=min_separation_s,\n",
    "                pre_s=0.5, post_s=window,\n",
    "                smooth_sec=smooth_sec\n",
    "            )\n",
    "\n",
    "            # optionally cap #events per unit\n",
    "            if max_events_per_unit is not None and len(events) > max_events_per_unit:\n",
    "                keep = np.sort(rng.choice(len(events), size=max_events_per_unit, replace=False))\n",
    "                events = [events[j] for j in keep]\n",
    "\n",
    "            aligned = []\n",
    "            rises, decays = [], []\n",
    "\n",
    "            for s, p, e in events:\n",
    "                seg_t = t[s:e]\n",
    "                seg_y = y[s:e]\n",
    "\n",
    "                kin = measure_event_kinetics(\n",
    "                    seg_t, seg_y, peak_idx=p - s, base_val=base,\n",
    "                    rise_lo=0.10, rise_hi=0.90,\n",
    "                    tail_to=tail_to, tail_frac=tail_frac,\n",
    "                    min_tail_pts=min_tail_pts, r2_min=r2_min,\n",
    "                    noise_k=noise_k, max_tau_s=(tau_cap if drop_capped else None)\n",
    "                )\n",
    "                # normalize segment for plotting (peak -> 1)\n",
    "                A = max(seg_y[p - s] - base, 1e-9)\n",
    "                aligned.append((seg_t - seg_t[p - s], (seg_y - base) / A))\n",
    "\n",
    "                # collect metrics (apply clip if we keep outliers)\n",
    "                if kin.get(\"ok_rise\", False) and np.isfinite(kin[\"rise_t10_90_s\"]):\n",
    "                    rises.append(float(kin[\"rise_t10_90_s\"]))\n",
    "\n",
    "                if kin.get(\"ok_tau\", False) and np.isfinite(kin[\"tau_decay_s\"]):\n",
    "                    tau = float(kin[\"tau_decay_s\"])\n",
    "                    if (tau_cap is not None) and (not drop_capped):\n",
    "                        tau = min(tau, float(tau_cap))\n",
    "                    decays.append(tau)\n",
    "\n",
    "            # accumulate across unit, across figure, across all\n",
    "            rise_all.extend(rises);  decay_all.extend(decays)\n",
    "            all_rise.extend(rises);  all_decay.extend(decays)\n",
    "\n",
    "            # --- PLOTTING ---\n",
    "            ax1, ax_rise, ax_decay = axes[i]\n",
    "\n",
    "            # 1) aligned transients\n",
    "            for tt, yy in aligned:\n",
    "                ax1.plot(tt, yy, lw=0.8, alpha=0.65)\n",
    "            ax1.axvline(0, color='k', lw=0.6)\n",
    "            ax1.set_xlim(-0.5, window)\n",
    "            ax1.set_ylim(0, 1.2)\n",
    "            ax1.set_title(f\"Unit {int(uid)}: aligned (n={len(aligned)})\")\n",
    "            ax1.set_xlabel(\"Time (s)\")\n",
    "\n",
    "            # 2) rise-only histogram\n",
    "            ax_rise.cla()\n",
    "            if len(rises):\n",
    "                r = np.asarray(rises, dtype=float)\n",
    "                r_max = max(0.2, float(np.nanpercentile(r, 99)))\n",
    "                ax_rise.hist(r, bins=np.linspace(0, r_max, 15), alpha=0.75, color=\"tab:blue\")\n",
    "                ax_rise.set_xlim(0, r_max)\n",
    "            ax_rise.set_title(\"Rise 10–90%\")\n",
    "            ax_rise.set_xlabel(\"Seconds\")\n",
    "\n",
    "            # 3) decay-only histogram\n",
    "            ax_decay.cla()\n",
    "            if len(decays):\n",
    "                d = np.asarray(decays, dtype=float)\n",
    "                d_max = max(0.2, float(np.nanpercentile(d, 99)))\n",
    "                ax_decay.hist(d, bins=np.linspace(0, d_max, 15), alpha=0.75, color=\"orange\")\n",
    "                ax_decay.set_xlim(0, d_max)\n",
    "            ax_decay.set_title(\"Decay τ\")\n",
    "            ax_decay.set_xlabel(\"Seconds\")\n",
    "\n",
    "        # helper for median ± IQR text\n",
    "        def _txt(x):\n",
    "            if not x: return \"n/a\"\n",
    "            q25, med, q75 = np.percentile(x, [25, 50, 75])\n",
    "            return f\"{med:.2f} s (IQR {q25:.2f}–{q75:.2f})\"\n",
    "\n",
    "        plt.suptitle(\n",
    "            f\"Group {fig_idx}  |  Rise: {_txt(rise_all)}   •   Decay: {_txt(decay_all)}\",\n",
    "            y=1.02\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # per-figure stats (optional)\n",
    "        if return_stats and summarize == \"per_group\":\n",
    "            def _stats(x):\n",
    "                if not x: return None, (None, None), 0\n",
    "                q25, med, q75 = np.percentile(x, [25, 50, 75])\n",
    "                return float(med), (float(q25), float(q75)), int(len(x))\n",
    "            r_med, r_iqr, r_n = _stats(rise_all)\n",
    "            d_med, d_iqr, d_n = _stats(decay_all)\n",
    "            summaries.append({\n",
    "                \"group\": fig_idx,\n",
    "                \"units\": list(map(int, group)),\n",
    "                \"rise_median\": r_med, \"rise_iqr\": r_iqr, \"n_rise\": r_n,\n",
    "                \"decay_median\": d_med, \"decay_iqr\": d_iqr, \"n_decay\": d_n,\n",
    "            })\n",
    "\n",
    "    # single aggregated stats (optional)\n",
    "    if return_stats and summarize == \"all\":\n",
    "        def _stats(x):\n",
    "            if not x: return None, (None, None), 0\n",
    "            q25, med, q75 = np.percentile(x, [25, 50, 75])\n",
    "            return float(med), (float(q25), float(q75)), int(len(x))\n",
    "        r_med, r_iqr, r_n = _stats(all_rise)\n",
    "        d_med, d_iqr, d_n = _stats(all_decay)\n",
    "        return [{\n",
    "            \"group\": \"all\",\n",
    "            \"units\": list(map(int, chosen_units)),\n",
    "            \"rise_median\": r_med, \"rise_iqr\": r_iqr, \"n_rise\": r_n,\n",
    "            \"decay_median\": d_med, \"decay_iqr\": d_iqr, \"n_decay\": d_n,\n",
    "        }]\n",
    "\n",
    "    return summaries if return_stats else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d103ea8-6c4c-4755-a4b6-ff6ab17be004",
   "metadata": {},
   "source": [
    "## Run rise & decay time constant functions on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24bffeca-2c75-42b6-b605-34b7951d576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG: {'n_examples': 3, 'random_seed': 0, 'max_events_per_unit': 50, 'window': 2.5, 'min_prom_sigma': 4, 'min_separation_s': 1.2, 'smooth_sec': None, 'tail_to': 'baseline', 'tail_frac': 0.2, 'min_tail_pts': 8, 'r2_min': 0.9, 'noise_k': 2.0, 'tau_cap': 3.0, 'drop_capped': True}\n",
      "STATS: [{'group': 'all', 'units': [9, 11, 10], 'rise_median': 0.3102955952162745, 'rise_iqr': (0.29367875722680914, 0.3337003992468901), 'n_rise': 34, 'decay_median': 0.6316911256056117, 'decay_iqr': (0.5651564764884391, 0.7992108721981664), 'n_decay': 11}]\n"
     ]
    }
   ],
   "source": [
    "fps = 20.0\n",
    "\n",
    "\n",
    "CONFIG = dict(\n",
    "    n_examples=3,              # number of random units\n",
    "    random_seed=0,              # random unit selcter on/off\n",
    "    max_events_per_unit=50,     # calcium events per unit\n",
    "    window=2.5,                 # time after peak include oin aligned plot\n",
    "\n",
    "    #Peak detection\n",
    "    min_prom_sigma=4,           # peak detection: minimum prominence in units of noise\n",
    "    min_separation_s=1.2,       # peak detection: minimum seperation between different events (1.2s)\n",
    "    smooth_sec=None,            # optinal pre smooting window lenght in s\n",
    "\n",
    "    #Decay fit\n",
    "    tail_to=\"baseline\",             # or \"frac\" fit decay: \"baseline\" (down to baseline) or \"frac\" (to a fraction of peak)\n",
    "    tail_frac=0.20,             # if tail_to=\"frac\": fraction of peak height (e.g. 0.3 = 30%)\n",
    "    min_tail_pts=8,             # minimum number of data points required in the decay tail for fitting\n",
    "    r2_min=0.9,                # minimum R² for exponential decay fit to be considered valid\n",
    "    noise_k=2.0,                # noise robustness factor for event detection (higher = stricter detection)\n",
    "\n",
    "    #Outlier control for stats\n",
    "    tau_cap=3.0,                # maximum allowed decay time constant [s] (anything longer will be clipped/dropped)\n",
    "    drop_capped=True           # True = drop events with τ > tau_cap, False = include but clip to tau_cap\n",
    ")\n",
    "\n",
    "stats = plot_transients_with_summary(\n",
    "    C, list(C.unit_id.values), fps=fps,\n",
    "    return_stats=True, summarize=\"all\",\n",
    "    **{k:v for k,v in CONFIG.items() if k in\n",
    "       [\"n_examples\",\"window\",\"min_prom_sigma\",\"min_separation_s\",\"smooth_sec\",\n",
    "        \"max_events_per_unit\",\"random_seed\",\"tau_cap\",\"drop_capped\"]}\n",
    ")\n",
    "\n",
    "print(\"CONFIG:\", CONFIG)\n",
    "print(\"STATS:\", stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c5fb359-3358-4132-b549-b894eb22f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tau_to_g(tau_d, tau_r, fps, in_seconds=False):\n",
    "    if in_seconds:\n",
    "        tau_d *= fps; tau_r *= fps\n",
    "    r1 = np.exp(-1.0/float(tau_d)); r2 = np.exp(-1.0/float(tau_r))\n",
    "    return np.array([r1 + r2, -r1*r2], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9713ad2-433f-46e5-b735-fe41b70ce242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decay median: 0.632 s = 12.6 frames\n",
      "Decay q25:    0.565 s = 11.3 frames\n",
      "Decay q75:    0.799 s = 16.0 frames\n",
      "Rise median:  0.310 s = 6.2 frames\n",
      "\n",
      "g_med  = [ 1.77507465 -0.78640034]  (median decay)\n",
      "g_slow = [ 1.79053082 -0.79955626]  (q75 decay → slower tail)\n",
      "g_fast = [ 1.76650532 -0.77910633]  (q25 decay → faster tail)\n"
     ]
    }
   ],
   "source": [
    "decay_med_s   = stats[0][\"decay_median\"]\n",
    "decay_q25_s, decay_q75_s = stats[0][\"decay_iqr\"]\n",
    "rise_med_s    = stats[0][\"rise_median\"]\n",
    "\n",
    "# seconds → frames\n",
    "to_frames = lambda s: max(1.0, s * fps)\n",
    "tau_d_med  = to_frames(decay_med_s)\n",
    "tau_d_q25  = to_frames(decay_q25_s)\n",
    "tau_d_q75  = to_frames(decay_q75_s)\n",
    "tau_r_med  = to_frames(rise_med_s)\n",
    "\n",
    "# compute AR(2) coefficients\n",
    "g_med  = tau_to_g(tau_d_med, tau_r_med, fps)   # balanced default\n",
    "g_slow = tau_to_g(tau_d_q75, tau_r_med, fps)   # longer decay\n",
    "g_fast = tau_to_g(tau_d_q25, tau_r_med, fps)   # shorter decay\n",
    "\n",
    "print(f\"Decay median: {decay_med_s:.3f} s = {tau_d_med:.1f} frames\")\n",
    "print(f\"Decay q25:    {decay_q25_s:.3f} s = {tau_d_q25:.1f} frames\")\n",
    "print(f\"Decay q75:    {decay_q75_s:.3f} s = {tau_d_q75:.1f} frames\")\n",
    "print(f\"Rise median:  {rise_med_s:.3f} s = {tau_r_med:.1f} frames\\n\")\n",
    "\n",
    "print(\"g_med  =\", g_med,  \" (median decay)\")\n",
    "print(\"g_slow =\", g_slow, \" (q75 decay → slower tail)\")\n",
    "print(\"g_fast =\", g_fast, \" (q25 decay → faster tail)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65e1f5-94b1-4942-82e6-46d0d8fd8329",
   "metadata": {},
   "source": [
    "The **median decay** is a robust central tendency but can be biased downward by noise and truncated events.\n",
    "\n",
    "Using the **q75 decay (upper quartile)** gives a slightly longer τ, ensuring the AR(2) model does not underestimate tail length.\n",
    "Prevents overfitting to noisy, fast falloffs\n",
    "\n",
    "rise = median, decay = q75 gives a balanced AR(2) model that avoids underestimating calcium dynamics while remaining robust to noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5dfb76-2284-4dd2-91b3-68bd2044bc45",
   "metadata": {},
   "source": [
    "# Deconvolve Low-level AR(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95216fc-ecaf-4c29-8112-efcbebdfaa6c",
   "metadata": {},
   "source": [
    "## c, s = oasisAR2 (y_in, g1, g2, s_min=smin_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f29ab94-cfc6-40c4-a027-b28ae17dfeb8",
   "metadata": {},
   "source": [
    "c= deconvolved calcium\n",
    "\n",
    "s= pruned spikes\n",
    "\n",
    "y= original raw trace\n",
    "\n",
    "g1,g2= rise and decay time\n",
    "\n",
    "s_min: built-in spike sparsity/threshold used by oasisAR2 (bigger ⇒ fewer spikes)\n",
    "\n",
    "- Fixed kinetics: uses your chosen rise/decay (via g)—no re-estimation\n",
    "- only optimizes spikes (and calcium), not the AR parameters\n",
    "- two levers—s_min inside OASIS and post_thresh after—to push more/less into the residual vs spikes\n",
    "- unit_specific_smin tame noisy units without sacrificing others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11e0c3b-c5fa-4ec3-a08d-c84c6f9e45ea",
   "metadata": {},
   "source": [
    "## just helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be304767-614f-4377-b27d-46477e2e4ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward recursion of an AR(2) calcium model:\n",
    "#-- reconstructed calcium trace c that would result from those spikes under the chosen AR(2) dynamics--\n",
    "def forward_ar2(s, g):\n",
    "    c = np.zeros_like(s, dtype=float)\n",
    "    for t in range(2, len(s)):\n",
    "        c[t] = g[0]*c[t-1] + g[1]*c[t-2] + s[t]\n",
    "    return c\n",
    "\n",
    "#_prune_spikes applies flexible thresholds (robust z-score, absolute amplitude, or quantiles) to remove small/noisy inferred spikes\n",
    "#clean up inferred spike train after deconvolution\n",
    "# \"z\": Computes a robust z-score of each spike relative to the median and MAD\n",
    "\n",
    "# \"z_abs\" : First tries the z-score rule, If that eliminates everything (too strict) or MAD≈0, \n",
    "#it falls back to a simple absolute rule: keep spikes ≥ frac * max(spike amplitude)\n",
    "\n",
    "# \"abs\" : spikes whose amplitude ≥ thr\n",
    "\n",
    "# \"q\" : Keep only the top (1-q) fraction of spikes, i.e. above the amplitude at quantile q\n",
    "# None: return spikes as is\n",
    "\n",
    "def _prune_spikes(s, post_thresh, debug=False):\n",
    "    if post_thresh is None:\n",
    "        return s\n",
    "\n",
    "    mode = post_thresh[0] if isinstance(post_thresh, tuple) else \"abs\"\n",
    "\n",
    "    if mode == \"z\":\n",
    "        z = float(post_thresh[1])\n",
    "        nz = s[s > 0]\n",
    "        if nz.size == 0:\n",
    "            if debug: print(\"[prune] z: no nonzero spikes -> keep none\")\n",
    "            return s*0\n",
    "        mu  = np.median(nz)\n",
    "        mad = np.median(np.abs(nz - mu))*1.4826\n",
    "        if mad > 1e-9:\n",
    "            keep = ((s - mu)/mad) >= z\n",
    "            if debug: print(f\"[prune] z: MAD={mad:.3g}, kept={keep.sum()} / {s.size}\")\n",
    "            return s*keep\n",
    "        if debug: print(\"[prune] z: MAD≈0 -> no-op\")\n",
    "        return s\n",
    "\n",
    "    if mode == \"z_abs\":\n",
    "        z, frac = float(post_thresh[1]), float(post_thresh[2])\n",
    "        nz = s[s > 0]\n",
    "        if nz.size == 0:\n",
    "            if debug: print(\"[prune] z_abs: no nonzero spikes -> keep none\")\n",
    "            return s*0\n",
    "        mu  = np.median(nz)\n",
    "        mad = np.median(np.abs(nz - mu))*1.4826\n",
    "        if mad > 1e-9:\n",
    "            keep = ((s - mu)/mad) >= z\n",
    "            if keep.sum() > 0:\n",
    "                if debug: print(f\"[prune] z_abs: z-kept={keep.sum()} / {s.size}\")\n",
    "                return s*keep\n",
    "            if debug: print(\"[prune] z_abs: z kept 0 -> fallback abs\")\n",
    "        else:\n",
    "            if debug: print(\"[prune] z_abs: MAD≈0 -> fallback abs\")\n",
    "        # absolute fallback\n",
    "        m = nz.max()\n",
    "        thr = frac * m\n",
    "        keep = s >= thr\n",
    "        if debug: print(f\"[prune] z_abs: abs thr={thr:.5g}, kept={keep.sum()} / {s.size}\")\n",
    "        return s*keep\n",
    "\n",
    "    if mode == \"abs\":\n",
    "        thr = float(post_thresh[1])\n",
    "        keep = s >= thr\n",
    "        if debug: print(f\"[prune] abs: thr={thr:.5g}, kept={keep.sum()} / {s.size}\")\n",
    "        return s*keep\n",
    "\n",
    "    if mode == \"q\":\n",
    "        q = float(post_thresh[1])\n",
    "        nz = s[s > 0]\n",
    "        if nz.size == 0:\n",
    "            if debug: print(\"[prune] q: no nonzero spikes -> keep none\")\n",
    "            return s*0\n",
    "        thr = np.quantile(nz, q)\n",
    "        keep = s >= thr\n",
    "        if debug: print(f\"[prune] q: q={q}, thr={thr:.5g}, kept={keep.sum()} / {s.size}\")\n",
    "        return s*keep\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79b680c2-0ba6-4445-84be-52e73c575135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#low-level OASIS AR(2) wrapper\n",
    "#deconvolution per unit with fixed kinetics, optional baseline subtraction, and optional post-hoc spike pruning\n",
    "def run_oasis_ar2(\n",
    "    C_da, unit_ids, g, s_min=1.0, baseline=\"p10\", fps=20.0,\n",
    "    post_thresh=(\"z_abs\", 2.5, 0.2),\n",
    "    unit_specific_smin=None,\n",
    "    debug_prune=False,\n",
    "    keep_unpruned=True,     # <-- NEW: keep s before pruning\n",
    "):\n",
    "    g = np.asarray(g, dtype=np.float64); g1, g2 = float(g[0]), float(g[1])\n",
    "    out = {}\n",
    "    for uid in unit_ids:\n",
    "        y = np.ascontiguousarray(C_da.sel(unit_id=int(uid)).values, dtype=np.float64)\n",
    "\n",
    "        # baseline\n",
    "        if isinstance(baseline, str) and baseline.startswith(\"p\"):\n",
    "            p = float(baseline[1:]); b = float(np.percentile(y, p))\n",
    "        elif isinstance(baseline, (int, float)):\n",
    "            b = float(baseline)\n",
    "        else:\n",
    "            b = 0.0\n",
    "\n",
    "        # unit-level s_min override\n",
    "        smin_u = unit_specific_smin.get(int(uid), s_min) if unit_specific_smin else s_min\n",
    "\n",
    "        # OASIS\n",
    "        c, s = oasisAR2(y - b, g1, g2, s_min=float(smin_u))\n",
    "\n",
    "        # keep a copy BEFORE pruning\n",
    "        s_unpruned = s.copy() if keep_unpruned else None\n",
    "\n",
    "        # post-prune spikes\n",
    "        s = _prune_spikes(s, post_thresh, debug=debug_prune)\n",
    "\n",
    "        out[int(uid)] = dict(\n",
    "            raw=y, c=c, s=s, s_unpruned=s_unpruned,  # <-- store both\n",
    "            b=b, g=np.array([g1, g2], dtype=np.float64),\n",
    "            time=np.arange(len(y), dtype=np.float64)/float(fps)\n",
    "        )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c15e541-b519-408a-97ef-b1a2d4161784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit_split_with_spikes(results, C_lp=None, start_s=200, duration_s=300,\n",
    "                               fps=20.0, normalize=False, spike_scale=\"auto\"):\n",
    "    \"\"\"\n",
    "    results: dict[unit_id] -> {\"raw\",\"c\",\"s\",\"time\",\"g\",...}\n",
    "    C_lp   : optional low-pass traces to overlay on Raw\n",
    "    spike_scale:\n",
    "        - \"auto\": scale spikes to ~max(c) in the shown window (per unit)\n",
    "        - float: use a fixed multiplier for spike heights (e.g., 1.0 keeps raw s)\n",
    "    \"\"\"\n",
    "    units = list(results.keys())\n",
    "    s0 = int(round(start_s * fps))\n",
    "    s1 = int(round((start_s + duration_s) * fps))\n",
    "    t  = np.arange(s0, s1) / fps\n",
    "\n",
    "    n = len(units)\n",
    "    fig, axes = plt.subplots(n, 4, figsize=(16, 2.4*n),\n",
    "                             gridspec_kw={\"wspace\": 0.25, \"hspace\": 0.35},\n",
    "                             sharex=True)\n",
    "    if n == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, uid in enumerate(units):\n",
    "        R = results[uid]\n",
    "        raw = np.asarray(R[\"raw\"])[s0:s1]\n",
    "        c   = np.asarray(R[\"c\"])[s0:s1]\n",
    "        s   = np.asarray(R[\"s\"])[s0:s1]\n",
    "\n",
    "        # reconvolved fit from spikes (same AR(2) as used in results)\n",
    "        g = np.asarray(R[\"g\"])\n",
    "        yhat = np.zeros_like(s, dtype=float)\n",
    "        for k in range(2, len(s)):\n",
    "            yhat[k] = g[0]*yhat[k-1] + g[1]*yhat[k-2] + s[k]\n",
    "\n",
    "        if normalize:\n",
    "            denom = np.nanmax(raw) + 1e-12\n",
    "            raw = raw / denom; c = c / denom; yhat = yhat / denom\n",
    "\n",
    "        # indices of spike samples\n",
    "        idx = np.flatnonzero(s > 0)\n",
    "        # spike scaling (used for the rightmost \"Spikes\" panel and to derive overlay heights)\n",
    "        if spike_scale == \"auto\":\n",
    "            cmax = float(np.nanmax(c)) if np.any(np.isfinite(c)) else 1.0\n",
    "            smax = float(np.nanmax(s)) if idx.size else 1.0\n",
    "            scale = (0.9 * cmax / smax) if smax > 0 else 1.0\n",
    "        else:\n",
    "            scale = float(spike_scale)\n",
    "\n",
    "        # --- Raw (±low-pass) ---\n",
    "        ax = axes[i, 0]\n",
    "        ax.plot(t, raw, color=\"0.25\", lw=0.8, label=\"Raw\")\n",
    "        if C_lp is not None:\n",
    "            ax.plot(t, C_lp.sel(unit_id=int(uid)).values[s0:s1],\n",
    "                    color=\"tab:blue\", lw=0.9, alpha=0.8, label=\"Low-pass\")\n",
    "        # overlay spikes on Raw (fixed-height stems relative to Raw range)\n",
    "        if idx.size:\n",
    "            rmin, rmax = float(np.nanmin(raw)), float(np.nanmax(raw))\n",
    "            rrange = max(1e-9, rmax - rmin)\n",
    "            base_raw = rmin + 0.02 * rrange            # start a bit above bottom\n",
    "            height_raw = 0.15 * rrange                 # ~15% panel height\n",
    "            ax.vlines(t[idx], base_raw, base_raw + height_raw, colors=\"tab:green\", lw=0.8, alpha=0.9)\n",
    "        if i == 0:\n",
    "            ax.set_title(\"Raw (±low-pass)\")\n",
    "            ax.legend(frameon=False, fontsize=8)\n",
    "        ax.set_ylabel(f\"Unit {uid}\")\n",
    "\n",
    "        # --- Deconvolved (c) ---\n",
    "        axc = axes[i, 1]\n",
    "        axc.plot(t, c, color=\"tab:purple\", lw=0.9)\n",
    "        # overlay spikes on Deconvolved (scaled to c range)\n",
    "        if idx.size:\n",
    "            cmin, cmax = float(np.nanmin(c)), float(np.nanmax(c))\n",
    "            crange = max(1e-9, cmax - cmin)\n",
    "            base_c = cmin + 0.03 * crange\n",
    "            height_c = 0.20 * crange                   # ~20% panel height\n",
    "            axc.vlines(t[idx], base_c, base_c + height_c, colors=\"tab:green\", lw=0.8, alpha=0.9)\n",
    "        if i == 0:\n",
    "            axc.set_title(\"Deconvolved (c)\")\n",
    "\n",
    "        # --- Fit & Residual ---\n",
    "        res = raw - yhat\n",
    "        axfr = axes[i, 2]\n",
    "        axfr.plot(t, yhat, color=\"tab:red\", lw=0.9, label=\"Reconvolved fit\")\n",
    "        axfr.plot(t, res,  color=\"0.6\",    lw=0.7, label=\"Residual\")\n",
    "        if i == 0:\n",
    "            axfr.set_title(\"Fit & Residual\")\n",
    "            axfr.legend(frameon=False, fontsize=8)\n",
    "\n",
    "        # --- Spikes (stick plot) ---\n",
    "        axsp = axes[i, 3]\n",
    "        if idx.size:\n",
    "            axsp.vlines(t[idx], 0, s[idx] * scale, colors=\"tab:green\", linewidth=0.8)\n",
    "        axsp.set_ylim(0, max(1e-6, (np.nanmax(s[idx] * scale) if idx.size else 1.0)))\n",
    "        if i == 0: axsp.set_title(\"Spikes (scaled)\")\n",
    "\n",
    "        # cosmetics\n",
    "        for a in axes[i, :]:\n",
    "            a.spines[\"top\"].set_visible(False)\n",
    "            a.spines[\"right\"].set_visible(False)\n",
    "            a.tick_params(length=3)\n",
    "        if i == n-1:\n",
    "            for a in axes[i, :]:\n",
    "                a.set_xlabel(\"Time (s)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26615222-72cc-43a2-99e2-1e4ef011f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_fit_split_with_spikes_plotly(results, C_lp=None, start_s=200, duration_s=300,\n",
    "                                      fps=20.0, normalize=False, spikes_height_frac=0.15,\n",
    "                                      cols=2, html_path=\"spikes_overlay.html\"):\n",
    "    units = list(results.keys())\n",
    "    s0 = int(round(start_s*fps)); s1 = int(round((start_s+duration_s)*fps))\n",
    "\n",
    "    rows = int(np.ceil(len(units)/cols))\n",
    "    fig = make_subplots(rows=rows, cols=cols,\n",
    "                        subplot_titles=[f\"Unit {u}\" for u in units],\n",
    "                        shared_xaxes=False, shared_yaxes=False)\n",
    "\n",
    "    for i, u in enumerate(units):\n",
    "        r = results[u]\n",
    "        t   = np.arange(s0, s1)/fps\n",
    "        raw = np.asarray(r[\"raw\"])[s0:s1]\n",
    "        c   = np.asarray(r[\"c\"])[s0:s1]\n",
    "        s   = np.asarray(r[\"s\"])[s0:s1]\n",
    "        idx = np.flatnonzero(s > 0)\n",
    "\n",
    "        if normalize:\n",
    "            denom = np.nanmax(raw) + 1e-12\n",
    "            raw = raw/denom; c = c/denom\n",
    "\n",
    "        r_i = i//cols + 1\n",
    "        c_i = i%cols + 1\n",
    "        showleg = (i == 0)  # only first unit contributes legend entries  # <<\n",
    "\n",
    "        # Raw (dark gray)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=t, y=raw, name=\"Raw\", mode=\"lines\",\n",
    "            line=dict(width=1.5, color=\"rgba(60,60,60,0.7)\"),  # alpha 0.7\n",
    "            showlegend=showleg\n",
    "        ), r_i, c_i)\n",
    "\n",
    "\n",
    "        # Optional low-pass (blue)\n",
    "        if C_lp is not None:\n",
    "            lp = C_lp.sel(unit_id=int(u)).values[s0:s1]\n",
    "            if normalize:\n",
    "                lp = lp / (np.nanmax(lp) + 1e-12)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=t, y=lp, name=\"Low-pass\", mode=\"lines\",            # <<\n",
    "                line=dict(width=1.2, color=\"rgba(30,120,180,0.8)\"),\n",
    "                showlegend=showleg                                    # <<\n",
    "            ), r_i, c_i)\n",
    "\n",
    "        # Deconvolved c (purple)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=t, y=c, name=\"Deconvolved c\", mode=\"lines\",\n",
    "            line=dict(width=1.5, color=\"rgba(148,0,211,0.7)\"), # alpha 0.7\n",
    "            showlegend=showleg\n",
    "        ), r_i, c_i)\n",
    "\n",
    "        # Spikes (green stems as shapes)\n",
    "        if idx.size:\n",
    "            ymin = float(np.nanmin(raw)); ymax = float(np.nanmax(raw))\n",
    "            yrng = max(1e-9, ymax - ymin)\n",
    "            y0 = ymin + 0.02*yrng; y1 = y0 + spikes_height_frac*yrng\n",
    "            for ti in t[idx]:\n",
    "                fig.add_shape(\n",
    "                    type=\"line\", x0=ti, x1=ti, y0=y0, y1=y1,\n",
    "                    line=dict(color=\"rgba(0,150,0,1)\", width=3),\n",
    "                    row=r_i, col=c_i\n",
    "                )\n",
    "        # Dummy trace so \"Spikes\" appears in legend once\n",
    "        if showleg:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[None], y=[None], mode=\"lines\",\n",
    "                line=dict(color=\"rgba(0,150,0,1)\", width=1.2),\n",
    "                name=\"Spikes\",\n",
    "                showlegend=True\n",
    "            ), r_i, c_i)\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=700*rows, width=1600,\n",
    "        showlegend=True,                                            # << turn legend on\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02,\n",
    "                    xanchor=\"left\", x=0),                           # << place legend on top\n",
    "        template=\"plotly_white\",\n",
    "        title=\"Raw + c + Spikes (interactive)\"\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Time (s)\")\n",
    "\n",
    "    fig.write_html(html_path, include_plotlyjs=\"cdn\", full_html=True)\n",
    "    print(f\"Saved interactive HTML to {html_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53b60d8a-8784-4097-a272-e06830907be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_spike_counts(results, fps=20.0, force_frames=None):\n",
    "    \"\"\"\n",
    "    results: dict[unit_id] -> {\"s\", \"s_unpruned\", \"time\", ...}\n",
    "    fps: frames/sec, used only if force_frames is provided instead of time\n",
    "    force_frames: optional int; if given, duration = force_frames / fps\n",
    "\n",
    "    Returns: (df_per_unit, totals_dict)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for uid, R in results.items():\n",
    "        s_after   = np.asarray(R[\"s\"])\n",
    "        s_before  = np.asarray(R.get(\"s_unpruned\")) if (\"s_unpruned\" in R and R[\"s_unpruned\"] is not None) else None\n",
    "\n",
    "        n_after  = int((s_after  > 0).sum())\n",
    "        n_before = int((s_before > 0).sum()) if s_before is not None else None\n",
    "\n",
    "        # duration (prefer time if present)\n",
    "        if force_frames is not None:\n",
    "            duration_s = force_frames / float(fps)\n",
    "        elif \"time\" in R and R[\"time\"] is not None:\n",
    "            duration_s = float(np.nanmax(R[\"time\"]) - np.nanmin(R[\"time\"]))\n",
    "        else:\n",
    "            duration_s = len(s_after) / float(fps)\n",
    "\n",
    "        duration_min = duration_s / 60.0 if duration_s > 0 else np.nan\n",
    "\n",
    "        rate_after  = n_after  / duration_min if duration_min > 0 else np.nan\n",
    "        rate_before = (n_before / duration_min) if (n_before is not None and duration_min > 0) else None\n",
    "\n",
    "        rows.append(dict(\n",
    "            unit_id=uid,\n",
    "            spikes_before=n_before,\n",
    "            spikes_after=n_after,\n",
    "            reduction=(None if n_before is None else (n_before - n_after)),\n",
    "            reduction_pct=(None if n_before in (None, 0) else 100.0*(n_before - n_after)/n_before),\n",
    "            minutes=duration_min,\n",
    "            rate_before_per_min=rate_before,\n",
    "            rate_after_per_min=rate_after,\n",
    "        ))\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"unit_id\").reset_index(drop=True)\n",
    "\n",
    "    # Overall totals (only where we have before)\n",
    "    has_before = df[\"spikes_before\"].notna()\n",
    "    total_before = int(df.loc[has_before, \"spikes_before\"].sum()) if has_before.any() else None\n",
    "    total_after  = int(df[\"spikes_after\"].sum())\n",
    "    total_redux  = (None if total_before is None else total_before - total_after)\n",
    "    total_redux_pct = (None if (total_before in (None, 0)) else 100.0*total_redux/total_before)\n",
    "\n",
    "    totals = dict(\n",
    "        total_units=len(df),\n",
    "        total_spikes_before=total_before,\n",
    "        total_spikes_after=total_after,\n",
    "        total_reduction=total_redux,\n",
    "        total_reduction_pct=total_redux_pct,\n",
    "        mean_rate_after_per_min=float(df[\"rate_after_per_min\"].mean()),\n",
    "        mean_rate_before_per_min=(float(df[\"rate_before_per_min\"].mean()) if has_before.any() else None),\n",
    "    )\n",
    "    return df, totals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4dc7e-aa8d-4563-8617-4099f7307aa6",
   "metadata": {},
   "source": [
    "## Run deconvolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892a799-113b-4e0c-9759-862a487140a4",
   "metadata": {},
   "source": [
    "### Choose here the appropiate g from the rise / decay estimation perfomed earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cae5ef0e-b4b0-4cd9-8fa1-5da5d6d59c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decay median: 0.632 s = 12.6 frames\n",
      "Decay q25 fast:    0.565 s = 11.3 frames\n",
      "Decay q75 slow:    0.799 s = 16.0 frames\n",
      "Rise median:  0.310 s = 6.2 frames\n",
      "\n",
      "g_med  = [ 1.77507465 -0.78640034]  (median decay)\n",
      "g_slow = [ 1.79053082 -0.79955626]  (q75 decay → slower tail)\n",
      "g_fast = [ 1.76650532 -0.77910633]  (q25 decay → faster tail)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Decay median: {decay_med_s:.3f} s = {tau_d_med:.1f} frames\")\n",
    "print(f\"Decay q25 fast:    {decay_q25_s:.3f} s = {tau_d_q25:.1f} frames\")\n",
    "print(f\"Decay q75 slow:    {decay_q75_s:.3f} s = {tau_d_q75:.1f} frames\")\n",
    "print(f\"Rise median:  {rise_med_s:.3f} s = {tau_r_med:.1f} frames\\n\")\n",
    "\n",
    "print(\"g_med  =\", g_med,  \" (median decay)\")\n",
    "print(\"g_slow =\", g_slow, \" (q75 decay → slower tail)\")\n",
    "print(\"g_fast =\", g_fast, \" (q25 decay → faster tail)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf8c12f-cb1f-44fc-90b7-f22b5b704e4d",
   "metadata": {},
   "source": [
    "# Change parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "155f22bd-385c-4a7f-bbda-58e5f6d1c676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running deconvolution on 3 curated units\n",
      "[prune] q: q=0.98, thr=8.8818e-16, kept=80 / 6000\n",
      "[prune] q: q=0.98, thr=1.1102e-16, kept=31 / 6000\n",
      "[prune] q: q=0.98, thr=8.2601e-16, kept=29 / 6000\n"
     ]
    }
   ],
   "source": [
    "curated_units = list(C.unit_id.values)\n",
    "print(f\"Running deconvolution on {len(curated_units)} curated units\")\n",
    "\n",
    "results_all = run_oasis_ar2(\n",
    "    C, curated_units, g_slow,\n",
    "    s_min=1.0,\n",
    "    baseline=\"p10\",\n",
    "    post_thresh=(\"q\",0.98),\n",
    "    #unit_specific_smin={13: 2.0, 14: 1.5 },   #  only affects unit 13,   # or add per-unit overrides if needed\n",
    "    fps=fps,\n",
    "    debug_prune=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fbde1a7-5f07-432d-a782-9adc2d8b2b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved interactive HTML to spikes_overlay.html\n"
     ]
    }
   ],
   "source": [
    "plot_fit_split_with_spikes_plotly(results_all, fps=20.0,\n",
    "                                  cols=2, html_path=\"spikes_overlay.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5df0f1-38db-48df-a756-5e57325446fd",
   "metadata": {},
   "source": [
    "## save deconvolution results (like minian as zarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b98f87a-19c4-4761-af56-db6cd0db23e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved zarr dataset: ./output//WL25_DEC1_oasis_deconv.zarr\n",
      "\n",
      "============================================================\n",
      "SAVE SUMMARY\n",
      "============================================================\n",
      "Animal: WL25\n",
      "Session: DEC1\n",
      "Units: 3\n",
      "Frames: 6000\n",
      "Arrays saved: C_deconv, S, S_unpruned\n",
      "Filename: WL25_DEC1_oasis_deconv.zarr\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takuya/Git/Wireless-Miniscope/.venv/lib/python3.12/site-packages/zarr/api/asynchronous.py:247: ZarrUserWarning:\n",
      "\n",
      "Consolidated metadata is currently not part in the Zarr format 3 specification. It may not be supported by other zarr implementations and may change in the future.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\\\n",
    "\n",
    "output_dir = \"./output/\"\n",
    "animal_name = \"WL25\"  # <-- Change this for each animal\n",
    "session_date = \"DEC1\"  # Optional: add session info\n",
    "\n",
    "unit_ids = curated_units\n",
    "\n",
    "# Extract arrays from results\n",
    "C_deconv = np.stack([results_all[uid][\"c\"] for uid in unit_ids])\n",
    "S = np.stack([results_all[uid][\"s\"] for uid in unit_ids])\n",
    "S_unpruned = np.stack([results_all[uid][\"s_unpruned\"] for uid in unit_ids])\n",
    "\n",
    "# Create xarray Dataset\n",
    "ds_oasis = xr.Dataset({\n",
    "    \"C_deconv\": ([\"unit_id\", \"frame\"], C_deconv.astype(np.float32)),\n",
    "    \"S\": ([\"unit_id\", \"frame\"], S.astype(np.float32)),\n",
    "    \"S_unpruned\": ([\"unit_id\", \"frame\"], S_unpruned.astype(np.float32))\n",
    "}, coords={\n",
    "    \"unit_id\": unit_ids,\n",
    "    \"frame\": np.arange(C_deconv.shape[1])\n",
    "})\n",
    "\n",
    "# Add metadata as attributes\n",
    "ds_oasis.attrs[\"fps\"] = fps\n",
    "ds_oasis.attrs[\"g_params\"] = g_slow.tolist()\n",
    "ds_oasis.attrs[\"s_min\"] = 1.0\n",
    "ds_oasis.attrs[\"baseline\"] = \"p10\"\n",
    "ds_oasis.attrs[\"post_thresh\"] = str((\"q\", 0.98))\n",
    "\n",
    "## Save to zarr - NOW includes session_date in filename\n",
    "zarr_filename = f\"{animal_name}_{session_date}_oasis_deconv.zarr\"\n",
    "ds_oasis.to_zarr(f\"{output_dir}/{zarr_filename}\", mode=\"w\")\n",
    "print(f\"✅ Saved zarr dataset: {output_dir}/{zarr_filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Animal: {animal_name}\")\n",
    "print(f\"Session: {session_date}\")\n",
    "print(f\"Units: {len(unit_ids)}\")\n",
    "print(f\"Frames: {C_deconv.shape[1]}\")\n",
    "print(f\"Arrays saved: C_deconv, S, S_unpruned\")\n",
    "print(f\"Filename: {zarr_filename}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad49a0-98a7-4114-b31d-657fdea6f186",
   "metadata": {},
   "source": [
    "## Plot normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e244a3-5a19-45e4-be96-c7e712a9d0f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (6000,) and (2000,)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m start_s = \u001b[32m200\u001b[39m\n\u001b[32m      2\u001b[39m duration_s = \u001b[32m300\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mplot_fit_split_with_spikes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_lp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_s\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration_s\u001b[49m\u001b[43m=\u001b[49m\u001b[43mduration_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspike_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m plt.savefig(\n\u001b[32m      8\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manimal_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_deconv_plots_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(start_s)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(start_s+duration_s)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms.png\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     dpi=\u001b[32m300\u001b[39m,\n\u001b[32m     10\u001b[39m     bbox_inches=\u001b[33m'\u001b[39m\u001b[33mtight\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Saved plot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manimal_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_deconv_plots_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(start_s)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(start_s+duration_s)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms.png\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mplot_fit_split_with_spikes\u001b[39m\u001b[34m(results, C_lp, start_s, duration_s, fps, normalize, spike_scale)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# --- Raw (±low-pass) ---\u001b[39;00m\n\u001b[32m     49\u001b[39m ax = axes[i, \u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m0.25\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlw\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRaw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m C_lp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     52\u001b[39m     ax.plot(t, C_lp.sel(unit_id=\u001b[38;5;28mint\u001b[39m(uid)).values[s0:s1],\n\u001b[32m     53\u001b[39m             color=\u001b[33m\"\u001b[39m\u001b[33mtab:blue\u001b[39m\u001b[33m\"\u001b[39m, lw=\u001b[32m0.9\u001b[39m, alpha=\u001b[32m0.8\u001b[39m, label=\u001b[33m\"\u001b[39m\u001b[33mLow-pass\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Wireless-Miniscope/.venv/lib/python3.12/site-packages/matplotlib/axes/_axes.py:1777\u001b[39m, in \u001b[36mAxes.plot\u001b[39m\u001b[34m(self, scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1534\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1535\u001b[39m \u001b[33;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[32m   1536\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1774\u001b[39m \u001b[33;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1776\u001b[39m kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n\u001b[32m-> \u001b[39m\u001b[32m1777\u001b[39m lines = [*\u001b[38;5;28mself\u001b[39m._get_lines(\u001b[38;5;28mself\u001b[39m, *args, data=data, **kwargs)]\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[32m   1779\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_line(line)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Wireless-Miniscope/.venv/lib/python3.12/site-packages/matplotlib/axes/_base.py:297\u001b[39m, in \u001b[36m_process_plot_var_args.__call__\u001b[39m\u001b[34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m     this += args[\u001b[32m0\u001b[39m],\n\u001b[32m    296\u001b[39m     args = args[\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/Wireless-Miniscope/.venv/lib/python3.12/site-packages/matplotlib/axes/_base.py:494\u001b[39m, in \u001b[36m_process_plot_var_args._plot_args\u001b[39m\u001b[34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[39m\n\u001b[32m    491\u001b[39m     axes.yaxis.update_units(y)\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.shape[\u001b[32m0\u001b[39m] != y.shape[\u001b[32m0\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx and y must have same first dimension, but \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.ndim > \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y.ndim > \u001b[32m2\u001b[39m:\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx and y can be no greater than 2D, but have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    498\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: x and y must have same first dimension, but have shapes (6000,) and (2000,)"
     ]
    }
   ],
   "source": [
    "start_s = 200\n",
    "duration_s = 300\n",
    "\n",
    "plot_fit_split_with_spikes(results_all, C_lp=None, start_s=start_s, duration_s=duration_s, fps=fps,\n",
    "                           normalize=True, spike_scale=\"auto\")\n",
    "\n",
    "plt.savefig(\n",
    "    f\"{output_dir}/{animal_name}_deconv_plots_{int(start_s)}-{int(start_s+duration_s)}s.png\",\n",
    "    dpi=300,\n",
    "    bbox_inches='tight'\n",
    ")\n",
    "print(f\"✅ Saved plot: {output_dir}/{animal_name}_deconv_plots_{int(start_s)}-{int(start_s+duration_s)}s.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ad438-1edc-4eb0-9977-936ed66acdc8",
   "metadata": {},
   "source": [
    "## Load saved deconvolved data\n",
    "### C_deconv - Deconvolved Calcium\n",
    "### S - Spike Train (pruned)\n",
    "### S_unpruned - Spike Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee22dc0-b164-40c3-b91d-92fc8fa39bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load saved deconvolved data\n",
    "#C_deconv - Deconvolved Calcium\n",
    "#S - Spike Train (pruned) like post oasis pruning  (\"q\", 0.98)\n",
    "#S_unpruned - Spike Train\n",
    "\n",
    "\n",
    "# Load the zarr file\n",
    "zarr_path = \"/Users/mbrosch/Documents/GitKraken_mac/OASIS/examples/WL3_JUL25_oasis_deconv.zarr\"\n",
    "ds_oasis = xr.open_zarr(zarr_path)\n",
    "\n",
    "print(ds_oasis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82d377-fe5f-45d2-b16a-f727f9a5a860",
   "metadata": {},
   "source": [
    "## Post result analysis: Residual RMS evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d4f97c-3ff5-4c63-9d5a-591c56b89d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"g_fast\": g_fast,\n",
    "    \"g_med\":  g_med,\n",
    "    \"g_slow\": g_slow,\n",
    "}\n",
    "\n",
    "def run_for_g(g):\n",
    "    return run_oasis_ar2(\n",
    "        C, curated_units, g,\n",
    "        s_min=1.0,\n",
    "        baseline=\"p10\",\n",
    "        post_thresh=None,          # no post-pruning while we compare\n",
    "        unit_specific_smin=None,\n",
    "        fps=fps,\n",
    "        debug_prune=False\n",
    "    )\n",
    "\n",
    "results_by_g = {name: run_for_g(g) for name, g in models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b548958-5a2d-4b88-a50a-3c94619b386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_rms(results, start_s=None, duration_s=None, fps=20.0):\n",
    "    \"\"\"Return (overall_mean_rms, per_unit_df). Windowing is optional.\"\"\"\n",
    "    rows = []\n",
    "    for uid, r in results.items():\n",
    "        raw = np.asarray(r[\"raw\"], dtype=float)\n",
    "        s   = np.asarray(r[\"s\"],   dtype=float)\n",
    "        yhat = forward_ar2(s, r[\"g\"])\n",
    "\n",
    "        if start_s is not None and duration_s is not None:\n",
    "            s0 = int(max(0, round(start_s * fps)))\n",
    "            s1 = int(min(len(raw), round((start_s + duration_s) * fps)))\n",
    "            raw = raw[s0:s1]; yhat = yhat[s0:s1]\n",
    "\n",
    "        res = raw - yhat\n",
    "        rms = float(np.sqrt(np.mean(res**2)))\n",
    "        rows.append({\"unit_id\": int(uid), \"rms\": rms})\n",
    "    df = pd.DataFrame(rows).sort_values(\"unit_id\")\n",
    "    return float(df[\"rms\"].mean()), df\n",
    "\n",
    "# compute RMS for each model\n",
    "rms_overall = {}\n",
    "rms_tables  = {}\n",
    "for name, res in results_by_g.items():\n",
    "    mean_rms, df = residual_rms(res, fps=fps)  # or add start_s/duration_s\n",
    "    rms_overall[name] = mean_rms\n",
    "    rms_tables[name]  = df\n",
    "\n",
    "print(\"Overall residual RMS:\")\n",
    "for k, v in rms_overall.items():\n",
    "    print(f\"  {k:6s}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda6f53a-8880-481c-8561-b601002a1e4a",
   "metadata": {},
   "source": [
    "## residuals g_fast might be low but just because its also overfitting noise and not signal per se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef5d99-1f3d-4888-88c9-620955691bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = rms_tables[\"g_med\"].rename(columns={\"rms\":\"rms_med\"})\n",
    "for k in (\"g_fast\",\"g_slow\"):\n",
    "    df_all = df_all.merge(rms_tables[k].rename(columns={\"rms\":f\"rms_{k.split('_')[1]}\"}),\n",
    "                          on=\"unit_id\", how=\"outer\")\n",
    "df_all[\"best_model\"] = df_all[[\"rms_fast\",\"rms_med\",\"rms_slow\"]].idxmin(axis=1)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26836993-8123-4e93-a06c-7bc28649b3c3",
   "metadata": {},
   "source": [
    "## Metric based QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68692bbc-ccea-490b-bb6d-5c4c6553eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===================== helpers =====================\n",
    "\n",
    "def _reconvolve_from_s(s, g):\n",
    "    \"\"\"Reconvolve spikes s with AR(2) kernel g=[g1,g2] to get fit yhat.\"\"\"\n",
    "    s = np.asarray(s, dtype=float)\n",
    "    yhat = np.zeros_like(s, dtype=float)\n",
    "    g = np.asarray(g, dtype=float)\n",
    "    for k in range(2, len(s)):\n",
    "        yhat[k] = g[0]*yhat[k-1] + g[1]*yhat[k-2] + s[k]\n",
    "    return yhat\n",
    "\n",
    "def _safe_corr(a, b):\n",
    "    a = np.asarray(a); b = np.asarray(b)\n",
    "    m = np.isfinite(a) & np.isfinite(b)\n",
    "    if m.sum() < 3: return np.nan\n",
    "    a = a[m] - np.nanmean(a[m]); b = b[m] - np.nanmean(b[m])\n",
    "    da = np.sqrt(np.nanvar(a)); db = np.sqrt(np.nanvar(b))\n",
    "    if da == 0 or db == 0: return np.nan\n",
    "    return float(np.dot(a, b) / (len(a) * da * db))\n",
    "\n",
    "def _explained_variance(y, yhat):\n",
    "    y = np.asarray(y); yhat = np.asarray(yhat)\n",
    "    m = np.isfinite(y) & np.isfinite(yhat)\n",
    "    if m.sum() < 3: return np.nan\n",
    "    var_y = np.nanvar(y[m])\n",
    "    if var_y <= 0: return np.nan\n",
    "    return float(1.0 - np.nanvar(y[m] - yhat[m]) / var_y)\n",
    "\n",
    "def _eventize_indices(idx, min_gap_frames):\n",
    "    \"\"\"Group sorted indices into clusters separated by > min_gap_frames.\"\"\"\n",
    "    if idx.size == 0: return []\n",
    "    splits = np.where(np.diff(idx) > min_gap_frames)[0] + 1\n",
    "    return np.split(idx, splits)\n",
    "\n",
    "def _count_calcium_events(trace, fps, z=2.5, min_gap_s=0.5):\n",
    "    \"\"\"\n",
    "    Simple, robust event finder on a calcium-like trace:\n",
    "    - threshold by median + z * MAD (positive side)\n",
    "    - merge above-threshold samples with a refractory gap\n",
    "    - pick the local max in each merged segment as the event\n",
    "    \"\"\"\n",
    "    x = np.asarray(trace, float)\n",
    "    med = np.nanmedian(x)\n",
    "    mad = np.nanmedian(np.abs(x - med))*1.4826\n",
    "    thr = med + z*mad if mad > 1e-12 else med + (np.nanmax(x)-med)*0.25\n",
    "    nz = np.flatnonzero(x > thr)\n",
    "    if nz.size == 0:\n",
    "        return 0, np.array([], dtype=int)\n",
    "    groups = _eventize_indices(nz, int(round(min_gap_s*fps)))\n",
    "    peaks = [g[np.argmax(x[g])] for g in groups]\n",
    "    return len(peaks), np.array(peaks, dtype=int)\n",
    "\n",
    "def _cluster_spike_frames(s, fps, min_gap_s=0.35, min_cluster_frac=0.10):\n",
    "    \"\"\"\n",
    "    Cluster non-zero spike frames into spike events separated by min_gap_s.\n",
    "    Drop tiny clusters whose summed amplitude < min_cluster_frac of the largest.\n",
    "    \"\"\"\n",
    "    s = np.asarray(s)\n",
    "    idx = np.flatnonzero(s > 0)\n",
    "    if idx.size == 0:\n",
    "        return 0, []\n",
    "\n",
    "    groups = _eventize_indices(idx, int(round(min_gap_s*fps)))\n",
    "\n",
    "    # remove tiny clusters by area\n",
    "    if len(groups) > 0 and (min_cluster_frac is not None) and (min_cluster_frac > 0):\n",
    "        areas = [float(s[g].sum()) for g in groups]\n",
    "        amax = max(areas) if areas else 0.0\n",
    "        thr = min_cluster_frac * amax\n",
    "        groups = [g for g, a in zip(groups, areas) if a >= thr]\n",
    "\n",
    "    return len(groups), groups\n",
    "\n",
    "# ===================== metrics + tagging =====================\n",
    "\n",
    "def qc_metrics_from_results_with_events(results_all, fps, start_s=None, duration_s=None,\n",
    "                                        use_lowpass=False, lowpass_da=None,\n",
    "                                        z_events=2.5, min_gap_event_s=0.5,\n",
    "                                        min_gap_spike_s=0.35, min_cluster_frac=0.10):\n",
    "    \"\"\"\n",
    "    Build a per-unit QC table with core & event-based metrics.\n",
    "    Returns a DataFrame indexed by unit_id.\n",
    "    \"\"\"\n",
    "    units = list(results_all.keys())\n",
    "    rows = []\n",
    "    for uid in units:\n",
    "        R = results_all[uid]\n",
    "        raw = np.asarray(R[\"raw\"])\n",
    "        s   = np.asarray(R[\"s\"])\n",
    "        g   = np.asarray(R[\"g\"])\n",
    "        n   = len(raw)\n",
    "\n",
    "        # analysis window\n",
    "        if start_s is not None and duration_s is not None:\n",
    "            s0 = max(0, int(round(start_s*fps)))\n",
    "            s1 = min(n, int(round((start_s+duration_s)*fps)))\n",
    "        else:\n",
    "            s0, s1 = 0, n\n",
    "        sl = slice(s0, s1)\n",
    "\n",
    "        raw_w = raw[sl]; s_w = s[sl]\n",
    "        yhat_w = _reconvolve_from_s(s_w, g)\n",
    "\n",
    "        base_for_fit = raw_w\n",
    "        if use_lowpass and (lowpass_da is not None):\n",
    "            try:\n",
    "                base_for_fit = np.asarray(lowpass_da.sel(unit_id=int(uid)).values)[sl]\n",
    "            except Exception:\n",
    "                pass  # fall back to raw_w\n",
    "\n",
    "        # core metrics\n",
    "        corr_fit = _safe_corr(base_for_fit, yhat_w)\n",
    "        r2 = _explained_variance(base_for_fit, yhat_w)\n",
    "        resid_rms_ratio = float(\n",
    "            np.sqrt(np.nanmean((base_for_fit - yhat_w)**2)) /\n",
    "            (np.sqrt(np.nanmean(base_for_fit**2)) + 1e-12)\n",
    "        )\n",
    "        frames = (s1 - s0)\n",
    "        minutes = frames / fps / 60.0\n",
    "        nonzero_frames = int(np.count_nonzero(s_w))\n",
    "        spikes_per_min = float(nonzero_frames / max(1e-9, minutes))\n",
    "        nonzero_pct = 100.0 * nonzero_frames / max(1, frames)\n",
    "\n",
    "        # event metrics\n",
    "        n_events, _ = _count_calcium_events(base_for_fit, fps,\n",
    "                                            z=z_events, min_gap_s=min_gap_event_s)\n",
    "        n_spike_clusters, _ = _cluster_spike_frames(\n",
    "            s_w, fps, min_gap_s=min_gap_spike_s, min_cluster_frac=min_cluster_frac\n",
    "        )\n",
    "\n",
    "        spike_frames_per_event = float(nonzero_frames / max(1, n_events))\n",
    "        spike_clusters_per_event = float(n_spike_clusters / max(1, n_events))\n",
    "\n",
    "        rows.append(dict(\n",
    "            unit_id=int(uid),\n",
    "            corr_fit=corr_fit,\n",
    "            r2=r2,\n",
    "            resid_rms_ratio=resid_rms_ratio,\n",
    "            spikes_per_min=spikes_per_min,\n",
    "            nonzero_frames=nonzero_frames,\n",
    "            nonzero_pct=nonzero_pct,\n",
    "            n_calcium_events=int(n_events),\n",
    "            n_spike_clusters=int(n_spike_clusters),\n",
    "            spike_frames_per_event=spike_frames_per_event,\n",
    "            spike_clusters_per_event=spike_clusters_per_event\n",
    "        ))\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"unit_id\").reset_index(drop=True)\n",
    "    return df.set_index(\"unit_id\")  # keep unit_id always visible\n",
    "\n",
    "def qc_tag_units_events(df,\n",
    "                        corr_keep=0.30, corr_discard=0.15,\n",
    "                        r2_keep=0.20,   r2_discard=0.10,\n",
    "                        resid_keep=0.80, resid_discard=0.95,\n",
    "                        rate_min=1.0,   rate_max=20.0,\n",
    "                        frames_per_event_max=6.0,       # relaxed vs 5.0\n",
    "                        clusters_per_event_max=2.5):    # relaxed vs 2.0\n",
    "    \"\"\"\n",
    "    Add 'qc_tag' column using core + event thresholds.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    good = (\n",
    "        (df[\"corr_fit\"].fillna(-1) >= corr_keep) |\n",
    "        (df[\"r2\"].fillna(-1)       >= r2_keep)\n",
    "    ) & (\n",
    "        df[\"resid_rms_ratio\"] <= resid_keep\n",
    "    ) & (\n",
    "        df[\"spikes_per_min\"].between(rate_min, rate_max)\n",
    "    ) & (\n",
    "        df[\"spike_frames_per_event\"]   <= frames_per_event_max\n",
    "    ) & (\n",
    "        df[\"spike_clusters_per_event\"] <= clusters_per_event_max\n",
    "    )\n",
    "\n",
    "    bad = (\n",
    "        (df[\"corr_fit\"].fillna(-1) < corr_discard) &\n",
    "        (df[\"r2\"].fillna(-1)       < r2_discard)\n",
    "    ) | (\n",
    "        df[\"resid_rms_ratio\"] >= resid_discard\n",
    "    ) | (\n",
    "        ~df[\"spikes_per_min\"].between(0.5, 60.0)   # pathological guardrails\n",
    "    ) | (\n",
    "        df[\"spike_frames_per_event\"]   > (frames_per_event_max*2)\n",
    "    ) | (\n",
    "        df[\"spike_clusters_per_event\"] > (clusters_per_event_max*2)\n",
    "    )\n",
    "\n",
    "    df[\"qc_tag\"] = np.where(bad, \"discard\", np.where(good, \"keep\", \"borderline\"))\n",
    "    return df\n",
    "\n",
    "def _soften_for_strong_units(df, corr_strong=0.70, r2_strong=0.50, resid_strong=0.70):\n",
    "    \"\"\"\n",
    "    If core metrics are very strong, upgrade borderline → keep.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    strong = (df[\"corr_fit\"] >= corr_strong) & (df[\"r2\"] >= r2_strong) & (df[\"resid_rms_ratio\"] <= resid_strong)\n",
    "    df.loc[strong & (df[\"qc_tag\"] == \"borderline\"), \"qc_tag\"] = \"keep\"\n",
    "    return df\n",
    "\n",
    "# ===================== top-level convenience =====================\n",
    "\n",
    "def run_qc(results_all, fps, start_s=None, duration_s=None,\n",
    "           use_lowpass=False, lowpass_da=None,\n",
    "           z_events=2.5, min_gap_event_s=0.5,\n",
    "           min_gap_spike_s=0.35, min_cluster_frac=0.10,\n",
    "           # tagging thresholds (NAc-friendly defaults)\n",
    "           corr_keep=0.30, corr_discard=0.15,\n",
    "           r2_keep=0.20,   r2_discard=0.10,\n",
    "           resid_keep=0.80, resid_discard=0.95,\n",
    "           rate_min=1.0,   rate_max=20.0,\n",
    "           frames_per_event_max=6.0,\n",
    "           clusters_per_event_max=2.5,\n",
    "           soften_strong=True):\n",
    "    \"\"\"\n",
    "    Compute metrics, tag units, (optionally soften strong ones), and return (df, summary).\n",
    "    \"\"\"\n",
    "    df = qc_metrics_from_results_with_events(\n",
    "        results_all, fps, start_s, duration_s,\n",
    "        use_lowpass, lowpass_da,\n",
    "        z_events, min_gap_event_s,\n",
    "        min_gap_spike_s=min_gap_spike_s, min_cluster_frac=min_cluster_frac\n",
    "    )\n",
    "    df = qc_tag_units_events(\n",
    "        df,\n",
    "        corr_keep=corr_keep, corr_discard=corr_discard,\n",
    "        r2_keep=r2_keep,     r2_discard=r2_discard,\n",
    "        resid_keep=resid_keep, resid_discard=resid_discard,\n",
    "        rate_min=rate_min, rate_max=rate_max,\n",
    "        frames_per_event_max=frames_per_event_max,\n",
    "        clusters_per_event_max=clusters_per_event_max\n",
    "    )\n",
    "    if soften_strong:\n",
    "        df = _soften_for_strong_units(df)\n",
    "\n",
    "    summary = {\n",
    "        \"keep\":       df.index[df.qc_tag == \"keep\"].tolist(),\n",
    "        \"borderline\": df.index[df.qc_tag == \"borderline\"].tolist(),\n",
    "        \"discard\":    df.index[df.qc_tag == \"discard\"].tolist(),\n",
    "        \"counts\":     df.qc_tag.value_counts().to_dict()\n",
    "    }\n",
    "    return df, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc1433b-dee1-4fd5-969c-5adc119e7fd3",
   "metadata": {},
   "source": [
    "- `corr_fit` Does the spike-based model capture the main transient structure ≥0.3–0.4\n",
    "- `r2` Fraction of variance in the raw trace explained by the fit. ≥0.2\n",
    "- `resid_rms_ratio` How much variance is left unexplained, <0.8 (residual smaller than raw → fit captured a lot)\n",
    "- `spikes_per_min` Biological plausibility of inferred firing rate\n",
    "- `nonzero_frames` How often spikes are being called,a unit with hundreds of “nonzero frames” but only a handful of true calcium events may be over-deconvolving noise\n",
    "\n",
    "\n",
    "- `spike_frames_per_event` High values mean the algorithm is “smearing” spikes over many frames instead of giving a compact even, close to 1-3 frames\n",
    "- `spike_clusters_per_event` High values mean the algorithm is “splitting” a single biological event into multiple inferred spikes, clsoe to one cluster per calcium event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff7e01-0b18-463e-a891-8b71b3ac992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qc, summary = run_qc(\n",
    "    results_all, fps=20.0,\n",
    "    start_s=200, duration_s=300,\n",
    "    use_lowpass=False,             # your raw is already low-pass\n",
    "    # (you can tweak any thresholds here if needed)\n",
    ")\n",
    "\n",
    "print(df_qc.round(3))   # unit_id is the index\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da56277-94dc-4e94-935f-6c35fe58f367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
